{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a615766-e0ab-43fd-a4bc-e9937f758a77",
   "metadata": {},
   "source": [
    "# Building classification model on CIFAR-10 multiclass dataset\n",
    "It is based on the convolutional neural network implemented with PyTorch API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acca56c4-d04d-4702-8db5-d149e13c0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6a238-d412-4976-8da3-37f44f4f45d7",
   "metadata": {},
   "source": [
    "## Device configuration and defining hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b99822e9-7bfa-434a-99c4-c95847676972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using GPU acceleration :)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "batch_size = 40\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb190937-af4b-4bdc-be9d-4f75bb6356f4",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7de903-35bc-43fc-a674-4340f246a7c9",
   "metadata": {},
   "source": [
    "Firstly let's define a set of transformations which I will apply on images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eaecdcc5-70a9-44b4-83a4-e5b5f7fd443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomResizedCrop(32),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fb090-fcaf-4930-987d-417cb5fcb1af",
   "metadata": {},
   "source": [
    "Then creating a train and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e0794d8-8a78-4a02-8c14-f0c4f414433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ddb234c-fc87-4096-8605-2ce1596323c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb7284-3fb7-45f3-91a6-e7b11463523e",
   "metadata": {},
   "source": [
    "Some facts about loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "135de458-6223-49bd-8377-818baee90fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 classes:\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "classes_test = test_dataset.classes\n",
    "assert len(classes) == len(classes_test), \"Underrepresented classes!\"\n",
    "\n",
    "print(f\"There are {len(classes)} classes:\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff9fc077-9092-406d-af09-b08114e60801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 examples in the training set.\n"
     ]
    }
   ],
   "source": [
    "train_size = train_dataset.data.shape[0]\n",
    "print(f\"There are {train_size} examples in the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e42b4632-e05c-493b-ba60-e031e2583ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000 examples in the test set.\n"
     ]
    }
   ],
   "source": [
    "test_size = test_dataset.data.shape[0]\n",
    "print(f\"There are {test_size} examples in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b5d19-d188-4f4d-aead-584b7c97276d",
   "metadata": {},
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "770eaeee-ab27-44e9-a2b4-fee0c1d34921",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18771a49-b46d-4f88-96d4-4c05e02acc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7fe35-e845-4c1c-bb15-aad0fe87421b",
   "metadata": {},
   "source": [
    "Visualisation of exemplary images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8a8929f7-37ce-43c9-aeae-d923ef275e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ada5fa8a470>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mini_batches = iter(train_loader)\n",
    "mini_batch = next(mini_batches)\n",
    "image = mini_batch[0][0].permute(2, 1, 0)\n",
    "image = image.numpy()\n",
    "inp = std * image + mean\n",
    "inp = np.clip(inp, 0, 1)\n",
    "image.shape\n",
    "inp.shape\n",
    "plt.imshow(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9026a0d8-87e6-4d58-93ad-a5efa920a10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.06666672,  0.2941177 ,  0.7490196 ],\n",
       "        [ 0.07450986,  0.30980396,  0.75686276],\n",
       "        [ 0.09803927,  0.33333337,  0.7647059 ],\n",
       "        ...,\n",
       "        [ 0.48235297,  0.48235297,  0.49803925],\n",
       "        [-0.0745098 , -0.0745098 , -0.05882353],\n",
       "        [-0.3490196 , -0.35686272, -0.3333333 ]],\n",
       "\n",
       "       [[ 0.07450986,  0.30196083,  0.75686276],\n",
       "        [ 0.082353  ,  0.30980396,  0.75686276],\n",
       "        [ 0.09803927,  0.33333337,  0.7647059 ],\n",
       "        ...,\n",
       "        [ 0.33333337,  0.32549024,  0.34901965],\n",
       "        [-0.17647058, -0.18431371, -0.1607843 ],\n",
       "        [-0.4352941 , -0.44313723, -0.41176468]],\n",
       "\n",
       "       [[ 0.082353  ,  0.30980396,  0.7647059 ],\n",
       "        [ 0.09019613,  0.3176471 ,  0.7647059 ],\n",
       "        [ 0.10588241,  0.3411765 ,  0.77254903],\n",
       "        ...,\n",
       "        [-0.08235294, -0.09019607, -0.06666666],\n",
       "        [-0.46666664, -0.47450978, -0.45098037],\n",
       "        [-0.6627451 , -0.67058825, -0.6392157 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.09803927,  0.32549024,  0.77254903],\n",
       "        [ 0.11372554,  0.3411765 ,  0.77254903],\n",
       "        [ 0.13725495,  0.36470592,  0.77254903],\n",
       "        ...,\n",
       "        [ 0.27843142,  0.34901965,  0.3803922 ],\n",
       "        [ 0.28627455,  0.35686278,  0.38823533],\n",
       "        [ 0.28627455,  0.35686278,  0.38823533]],\n",
       "\n",
       "       [[ 0.09803927,  0.32549024,  0.77254903],\n",
       "        [ 0.10588241,  0.3411765 ,  0.77254903],\n",
       "        [ 0.12941182,  0.36470592,  0.7647059 ],\n",
       "        ...,\n",
       "        [-0.02745098,  0.03529418,  0.06666672],\n",
       "        [-0.03529412,  0.0196079 ,  0.05882359],\n",
       "        [-0.04313725,  0.01176476,  0.05098045]],\n",
       "\n",
       "       [[ 0.09803927,  0.32549024,  0.77254903],\n",
       "        [ 0.10588241,  0.3411765 ,  0.77254903],\n",
       "        [ 0.12941182,  0.36470592,  0.7647059 ],\n",
       "        ...,\n",
       "        [-0.1372549 , -0.0745098 , -0.04313725],\n",
       "        [-0.15294117, -0.09803921, -0.06666666],\n",
       "        [-0.1607843 , -0.10588235, -0.0745098 ]]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e23dea-a495-49f5-8684-35b1d0cfc947",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building CNN Model\n",
    "Firstly I wil try training model from the begining. Two convolutional layers with maxpooling followed by fully connected 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a4115f9-9711-4207-b1d7-a39f3ad7947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) #(in_channels, out_channels, kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2) #\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2ed1b7b-b599-490c-a225-7a6ddc9ae026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c35b12ff-f308-4501-94ea-5d7bab4b1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d98654-1efe-4881-96b7-49d9809b05df",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1d4a736-1fea-43e8-8064-08fadb7e8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Step: 500/1250, Loss: 1.6062\n",
      "Epoch: 1/10, Step: 1000/1250, Loss: 1.3311\n",
      "Epoch: 2/10, Step: 500/1250, Loss: 1.5644\n",
      "Epoch: 2/10, Step: 1000/1250, Loss: 1.1216\n",
      "Epoch: 3/10, Step: 500/1250, Loss: 1.3293\n",
      "Epoch: 3/10, Step: 1000/1250, Loss: 1.4173\n",
      "Epoch: 4/10, Step: 500/1250, Loss: 1.0742\n",
      "Epoch: 4/10, Step: 1000/1250, Loss: 1.3378\n",
      "Epoch: 5/10, Step: 500/1250, Loss: 1.0384\n",
      "Epoch: 5/10, Step: 1000/1250, Loss: 1.1379\n",
      "Epoch: 6/10, Step: 500/1250, Loss: 1.3039\n",
      "Epoch: 6/10, Step: 1000/1250, Loss: 0.9081\n",
      "Epoch: 7/10, Step: 500/1250, Loss: 1.0259\n",
      "Epoch: 7/10, Step: 1000/1250, Loss: 0.8653\n",
      "Epoch: 8/10, Step: 500/1250, Loss: 1.0190\n",
      "Epoch: 8/10, Step: 1000/1250, Loss: 1.0630\n",
      "Epoch: 9/10, Step: 500/1250, Loss: 0.4741\n",
      "Epoch: 9/10, Step: 1000/1250, Loss: 0.8329\n",
      "Epoch: 10/10, Step: 500/1250, Loss: 1.1568\n",
      "Epoch: 10/10, Step: 1000/1250, Loss: 0.9841\n",
      "Training Finished. Hurraay!\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader) # number of training mini-batches\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # mini-batch is sent to the device \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{n_total_steps}, Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training Finished. Hurraay!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc88fb-c3b0-4106-802d-bd925457c255",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd4ec9a5-cf11-4d9c-b96f-d9cb1f085c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 63.67%\n",
      "Accuracy of plane: 71.1%\n",
      "Accuracy of car: 82.4%\n",
      "Accuracy of bird: 55.9%\n",
      "Accuracy of cat: 43.6%\n",
      "Accuracy of deer: 56.7%\n",
      "Accuracy of dog: 38.1%\n",
      "Accuracy of frog: 73.1%\n",
      "Accuracy of horse: 75.3%\n",
      "Accuracy of ship: 72.4%\n",
      "Accuracy of truck: 68.1%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc}%')\n",
    "    \n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc}%')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2fde7-b300-4409-a016-3ca3e0a0edc6",
   "metadata": {},
   "source": [
    "## Building model by transfer learning\n",
    "I am using pretrained ResNet18 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfcb771-f690-4a97-9387-1cf76532c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd26be1-be87-4deb-9115-fa476922205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "684b0ec1-a6ac-4c55-8dff-09db124f53d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 45.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_size = (3, 32, 32)\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f0765a3-d817-49b7-8184-6a660428d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input size for the last fully connected layer. \n",
    "num_ftrs = model.fc.in_features\n",
    "num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f66eac3-8e27-45f0-a972-6ea4471240bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign new fully connected layer at the top of the ResNet18 model.\n",
    "n_classes = len(classes)\n",
    "model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2331794c-0eb9-4fdb-a3d9-82db6aca4826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "150cc63f-0e9c-49e2-9d85-280eb7851e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now definition of criterion/loss function\n",
    "criterion_RsN = nn.CrossEntropyLoss()\n",
    "\n",
    "# and optimizer definition\n",
    "optimizer_RsN = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Updating learning rate by scheduler\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer_RsN, step_size=7, gamma=0.1)\n",
    "# -> every 7 epochs learning rate is multiplied by gamma value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af52ce94-d23f-4afd-9b73-a0bd2a492e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Step: 500/1250, Loss: 1.0320\n",
      "Epoch: 1/10, Step: 1000/1250, Loss: 0.9050\n",
      "Epoch: 2/10, Step: 500/1250, Loss: 0.7939\n",
      "Epoch: 2/10, Step: 1000/1250, Loss: 0.7465\n",
      "Epoch: 3/10, Step: 500/1250, Loss: 0.5550\n",
      "Epoch: 3/10, Step: 1000/1250, Loss: 0.5983\n",
      "Epoch: 4/10, Step: 500/1250, Loss: 0.5494\n",
      "Epoch: 4/10, Step: 1000/1250, Loss: 0.6097\n",
      "Epoch: 5/10, Step: 500/1250, Loss: 0.3816\n",
      "Epoch: 5/10, Step: 1000/1250, Loss: 0.1740\n",
      "Epoch: 6/10, Step: 500/1250, Loss: 0.3619\n",
      "Epoch: 6/10, Step: 1000/1250, Loss: 0.1413\n",
      "Epoch: 7/10, Step: 500/1250, Loss: 0.1473\n",
      "Epoch: 7/10, Step: 1000/1250, Loss: 0.2289\n",
      "Epoch: 8/10, Step: 500/1250, Loss: 0.2712\n",
      "Epoch: 8/10, Step: 1000/1250, Loss: 0.1283\n",
      "Epoch: 9/10, Step: 500/1250, Loss: 0.2985\n",
      "Epoch: 9/10, Step: 1000/1250, Loss: 0.1755\n",
      "Epoch: 10/10, Step: 500/1250, Loss: 0.2019\n",
      "Epoch: 10/10, Step: 1000/1250, Loss: 0.0719\n",
      "Training Finished. Hurraay!\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader) # number of training mini-batches\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # mini-batch is sent to the device \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion_RsN(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer_RsN.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_RsN.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{n_total_steps}, Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training Finished. Hurraay!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4075a23f-d224-42ba-ac92-ad5f24456030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 79.1%\n",
      "Accuracy of plane: 81.7%\n",
      "Accuracy of car: 87.4%\n",
      "Accuracy of bird: 74.2%\n",
      "Accuracy of cat: 55.1%\n",
      "Accuracy of deer: 81.6%\n",
      "Accuracy of dog: 71.4%\n",
      "Accuracy of frog: 83.6%\n",
      "Accuracy of horse: 83.0%\n",
      "Accuracy of ship: 86.2%\n",
      "Accuracy of truck: 86.8%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc}%')\n",
    "    \n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f3053-5129-45d3-bb8b-50c5dae2ec47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
